{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Om4K260UoW5Y"
   },
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HHHkOoaIXf-z"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional,Conv1D,MaxPooling1D,Flatten\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1hR-5ROocEX"
   },
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "LGqY8GQScYu-",
    "outputId": "ed1f454b-0477-400a-8fd9-189e0d0cdc2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "data_2 = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "data =  pd.concat([data_1, data_2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-6TDR1oeY2"
   },
   "source": [
    "# Text Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jNsxUzOOcgsg"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)        \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text) \n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"did't\", \"did not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"have't\", \"have not\", text)\n",
    "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk1vcaksc2Uq",
    "outputId": "e8ee3cf7-745f-4c65-bb62-d39aa8b55b9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['former',\n",
       "  'versace',\n",
       "  'store',\n",
       "  'clerk',\n",
       "  'sues',\n",
       "  'secret',\n",
       "  'black',\n",
       "  'code',\n",
       "  'minority',\n",
       "  'shoppers'],\n",
       " ['roseanne',\n",
       "  'revival',\n",
       "  'catches',\n",
       "  'thorny',\n",
       "  'political',\n",
       "  'mood',\n",
       "  'better',\n",
       "  'worse'],\n",
       " ['mom',\n",
       "  'starting',\n",
       "  'fear',\n",
       "  'sons',\n",
       "  'web',\n",
       "  'series',\n",
       "  'closest',\n",
       "  'thing',\n",
       "  'grandchild'],\n",
       " ['boehner',\n",
       "  'wants',\n",
       "  'wife',\n",
       "  'listen',\n",
       "  'come',\n",
       "  'alternative',\n",
       "  'debtreduction',\n",
       "  'ideas'],\n",
       " ['jk', 'rowling', 'wishes', 'snape', 'happy', 'birthday', 'magical', 'way'],\n",
       " ['advancing', 'worlds', 'women'],\n",
       " ['fascinating', 'case', 'eating', 'labgrown', 'meat'],\n",
       " ['ceo', 'send', 'kids', 'school', 'work', 'company'],\n",
       " ['top', 'snake', 'handler', 'leaves', 'sinking', 'huckabee', 'campaign'],\n",
       " ['fridays', 'morning', 'email', 'inside', 'trumps', 'presser', 'ages']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def CleanTokenize(df):\n",
    "    head_lines = list()\n",
    "    lines = df[\"headline\"].values.tolist()\n",
    "\n",
    "    for line in lines:\n",
    "        line = clean_text(line)\n",
    "        # tokenize the text\n",
    "        tokens = word_tokenize(line)\n",
    "        # remove puntuations\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove non alphabetic characters\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        # remove stop words\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        head_lines.append(words)\n",
    "    return head_lines\n",
    "\n",
    "head_lines = CleanTokenize(data)\n",
    "head_lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lengths = [len(x) for x in head_lines]\n",
    "max_length_sentence = max(all_lengths)\n",
    "max_length_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGX52KZoorKO"
   },
   "source": [
    "# Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "na6GDnQ8dHlD",
    "outputId": "7da74a1f-dbf9-452c-fc0b-933261f1632f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens -  28657\n",
      "vocab size - 28658\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.2\n",
    "max_length = max_length_sentence\n",
    "\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(head_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(head_lines)\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "print(\"unique tokens - \",len(word_index))\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "print('vocab size -', vocab_size)\n",
    "\n",
    "lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "sentiment =  data['is_sarcastic'].values\n",
    "\n",
    "indices = np.arange(lines_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "lines_pad = lines_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "\n",
    "num_validation_samples = int(validation_split * lines_pad.shape[0])\n",
    "\n",
    "X_train_pad = lines_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = lines_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jap48K3oeI3L",
    "outputId": "d70b2cf4-4b43-4686-f488-bbed805f1960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad: (44263, 106)\n",
      "Shape of y_train: (44263,)\n",
      "Shape of X_test_pad: (11065, 106)\n",
      "Shape of y_test: (11065,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train_pad:', X_train_pad.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad:', X_test_pad.shape)\n",
    "print('Shape of y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJiFy679o0hW"
   },
   "source": [
    "# Importing Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc5VIcA618V8",
    "outputId": "fd22aa5f-ca77-4ee1-a2ef-65706f9af6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
      "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
     ]
    }
   ],
   "source": [
    "# ! pip install kaggle\n",
    "# ! mkdir ~/.kaggle\n",
    "# ! cp kaggle.json ~/.kaggle/\n",
    "# ! chmod 600 ~/.kaggle/kaggle.json #Your kaggle.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YndB-AZNfykG",
    "outputId": "a51e5d11-7969-46a1-8078-354333b80896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading glovetwitter27b100dtxt.zip to /content\n",
      " 96% 382M/397M [00:03<00:00, 145MB/s]\n",
      "100% 397M/397M [00:03<00:00, 126MB/s]\n"
     ]
    }
   ],
   "source": [
    "# ! kaggle datasets download -d bertcarremans/glovetwitter27b100dtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByrLudCqg23F",
    "outputId": "e74fb165-e150-4d6a-d574-4034dfc993be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glovetwitter27b100dtxt.zip\n",
      "  inflating: glove.twitter.27B.100d.txt  \n"
     ]
    }
   ],
   "source": [
    "# ! unzip glovetwitter27b100dtxt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o24JVBJ3o4SO"
   },
   "source": [
    "# Loading GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UC_sDSnUiRIt",
    "outputId": "06a12484-aee9-41d0-d908-9a41937efcf7"
   },
   "outputs": [],
   "source": [
    "# embeddings_index = {}\n",
    "# embedding_dim = 100\n",
    "# GLOVE_DIR = \"\"\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.twitte+r.27B.100d.txt'), encoding = \"utf-8\")\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Load Already Saved GloVe Model from Local\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"glove.twitter.27B.100d.txt.word2vec\",binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMGNhiSEo86W"
   },
   "source": [
    "# Creating Embedding Matrix + Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28657"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rca6TCr2iehL",
    "outputId": "638abadd-16d0-499f-a9cd-9687b957ab3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24755\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "c = 0\n",
    "for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "    try:\n",
    "        embedding_vector = model.get_vector(word)\n",
    "    except:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        c+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"EmbeddingMatrixGloVe.pkl\",'wb') as f:\n",
    "    pkl.dump([embedding_dim,embedding_matrix],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r4np1zNkjb6F"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NJf84B8pA3V"
   },
   "source": [
    "# Creating the main Neural Network with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hY4FHLDRjfT-",
    "outputId": "e5a72abd-106c-47f2-e50e-d32c08480502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 100)           2865800   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,908,105\n",
      "Trainable params: 42,305\n",
      "Non-trainable params: 2,865,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEixRIrkpFRw"
   },
   "source": [
    "#Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCKrBdVxjjOH",
    "outputId": "b585d780-07d3-4c99-ab23-c7acbbaf29ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1384/1384 [==============================] - 33s 22ms/step - loss: 0.5078 - accuracy: 0.7513 - val_loss: 0.4360 - val_accuracy: 0.7978\n",
      "Epoch 2/25\n",
      "1384/1384 [==============================] - 31s 22ms/step - loss: 0.4248 - accuracy: 0.8044 - val_loss: 0.3804 - val_accuracy: 0.8300\n",
      "Epoch 3/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.3894 - accuracy: 0.8247 - val_loss: 0.3620 - val_accuracy: 0.8395\n",
      "Epoch 4/25\n",
      "1384/1384 [==============================] - 31s 23ms/step - loss: 0.3578 - accuracy: 0.8410 - val_loss: 0.3398 - val_accuracy: 0.8542\n",
      "Epoch 5/25\n",
      "1384/1384 [==============================] - 31s 22ms/step - loss: 0.3343 - accuracy: 0.8549 - val_loss: 0.3282 - val_accuracy: 0.8596\n",
      "Epoch 6/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.3177 - accuracy: 0.8628 - val_loss: 0.3122 - val_accuracy: 0.8680\n",
      "Epoch 7/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.2987 - accuracy: 0.8740 - val_loss: 0.2998 - val_accuracy: 0.8728\n",
      "Epoch 8/25\n",
      "1384/1384 [==============================] - 31s 23ms/step - loss: 0.2870 - accuracy: 0.8788 - val_loss: 0.2980 - val_accuracy: 0.8781\n",
      "Epoch 9/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.2753 - accuracy: 0.8859 - val_loss: 0.2827 - val_accuracy: 0.8838\n",
      "Epoch 10/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.2632 - accuracy: 0.8900 - val_loss: 0.2741 - val_accuracy: 0.8900\n",
      "Epoch 11/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.2525 - accuracy: 0.8958 - val_loss: 0.2731 - val_accuracy: 0.8925\n",
      "Epoch 12/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.2437 - accuracy: 0.8989 - val_loss: 0.2639 - val_accuracy: 0.8943\n",
      "Epoch 13/25\n",
      "1384/1384 [==============================] - 33s 24ms/step - loss: 0.2364 - accuracy: 0.9015 - val_loss: 0.2538 - val_accuracy: 0.8987\n",
      "Epoch 14/25\n",
      "1384/1384 [==============================] - 33s 24ms/step - loss: 0.2323 - accuracy: 0.9040 - val_loss: 0.2487 - val_accuracy: 0.8991\n",
      "Epoch 15/25\n",
      "1384/1384 [==============================] - 33s 24ms/step - loss: 0.2231 - accuracy: 0.9087 - val_loss: 0.2518 - val_accuracy: 0.9034\n",
      "Epoch 16/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.2189 - accuracy: 0.9101 - val_loss: 0.2373 - val_accuracy: 0.9064\n",
      "Epoch 17/25\n",
      "1384/1384 [==============================] - 33s 24ms/step - loss: 0.2119 - accuracy: 0.9134 - val_loss: 0.2369 - val_accuracy: 0.9113\n",
      "Epoch 18/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.2073 - accuracy: 0.9168 - val_loss: 0.2300 - val_accuracy: 0.9159\n",
      "Epoch 19/25\n",
      "1384/1384 [==============================] - 33s 24ms/step - loss: 0.2061 - accuracy: 0.9157 - val_loss: 0.2412 - val_accuracy: 0.9047\n",
      "Epoch 20/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.2002 - accuracy: 0.9184 - val_loss: 0.2198 - val_accuracy: 0.9176\n",
      "Epoch 21/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.1986 - accuracy: 0.9201 - val_loss: 0.2300 - val_accuracy: 0.9153\n",
      "Epoch 22/25\n",
      "1384/1384 [==============================] - 33s 24ms/step - loss: 0.1938 - accuracy: 0.9217 - val_loss: 0.2204 - val_accuracy: 0.9166\n",
      "Epoch 23/25\n",
      "1384/1384 [==============================] - 34s 24ms/step - loss: 0.1901 - accuracy: 0.9233 - val_loss: 0.2170 - val_accuracy: 0.9173\n",
      "Epoch 24/25\n",
      "1384/1384 [==============================] - 32s 23ms/step - loss: 0.1890 - accuracy: 0.9245 - val_loss: 0.2203 - val_accuracy: 0.9207\n",
      "Epoch 25/25\n",
      "1384/1384 [==============================] - 33s 24ms/step - loss: 0.1876 - accuracy: 0.9244 - val_loss: 0.2065 - val_accuracy: 0.9260\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_pad, y_train, batch_size=32, epochs=25, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXpdstaRpIDF"
   },
   "source": [
    "#Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Szece5wH22bx"
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWTzGTLcpKW2"
   },
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0mkz-O-E8dyc"
   },
   "outputs": [],
   "source": [
    "def predict_sarcasm(s):\n",
    "    x_final = pd.DataFrame({\"headline\":[s]})\n",
    "    test_lines = CleanTokenize(x_final)\n",
    "    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
    "    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "    pred = model.predict(test_review_pad)\n",
    "    pred*=100\n",
    "    if pred[0][0]>=50: return \"It's a sarcasm!\" \n",
    "    else: return \"It's not a sarcasm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "cSSvRGBn8oG5",
    "outputId": "f5697547-963d-4391-ea78-d5872b1e35e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 351ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's a sarcasm!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"I was depressed. He asked me to be happy. I am not depressed anymore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "zhlOWpeE8pfL",
    "outputId": "de6f007a-74da-4b87-9a5b-b6cb712386da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"You just broke my car window. Get out from here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "todCm7-18rdL",
    "outputId": "0a545db5-6cda-4306-d52e-3770d6918045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"You just saved my dog's life. Sorry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-7BqI1og8yUr",
    "outputId": "9faf40f1-d508-4251-86e3-348e1f6e695e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"I want a million dollars!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "9_ezOzCV9BIV",
    "outputId": "bb5cf56b-b6a0-47ff-9b48-645be47118db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's a sarcasm!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"I just won a million dollars!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jiOVbkeg9CuH",
    "outputId": "86a5f3e0-c6bb-4dbe-db48-d497f7aaf4ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346/346 [==============================] - 3s 8ms/step - loss: 0.2065 - accuracy: 0.9260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20645006000995636, 0.9259828329086304]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1fjiemj9nql"
   },
   "source": [
    "# Training the model with Embedding + CNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 100)           2865800   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 18, 32)            25632     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 9, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               28900     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,920,433\n",
      "Trainable params: 54,633\n",
      "Non-trainable params: 2,865,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(embedding_layer)\n",
    "cnn.add(Conv1D(filters=32,kernel_size=8,activation='relu'))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(100, activation='relu'))\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1384/1384 [==============================] - 15s 11ms/step - loss: 0.4677 - accuracy: 0.7760 - val_loss: 0.3985 - val_accuracy: 0.8217\n",
      "Epoch 2/25\n",
      "1384/1384 [==============================] - 17s 12ms/step - loss: 0.3249 - accuracy: 0.8611 - val_loss: 0.3335 - val_accuracy: 0.8558\n",
      "Epoch 3/25\n",
      "1384/1384 [==============================] - 17s 12ms/step - loss: 0.2239 - accuracy: 0.9108 - val_loss: 0.3030 - val_accuracy: 0.8787\n",
      "Epoch 4/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.1562 - accuracy: 0.9413 - val_loss: 0.2728 - val_accuracy: 0.9096\n",
      "Epoch 5/25\n",
      "1384/1384 [==============================] - 17s 12ms/step - loss: 0.1108 - accuracy: 0.9604 - val_loss: 0.2664 - val_accuracy: 0.9195\n",
      "Epoch 6/25\n",
      "1384/1384 [==============================] - 13s 9ms/step - loss: 0.0841 - accuracy: 0.9707 - val_loss: 0.3185 - val_accuracy: 0.9172\n",
      "Epoch 7/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0670 - accuracy: 0.9763 - val_loss: 0.3187 - val_accuracy: 0.9253\n",
      "Epoch 8/25\n",
      "1384/1384 [==============================] - 16s 11ms/step - loss: 0.0578 - accuracy: 0.9788 - val_loss: 0.3040 - val_accuracy: 0.9348\n",
      "Epoch 9/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0465 - accuracy: 0.9834 - val_loss: 0.3462 - val_accuracy: 0.9347\n",
      "Epoch 10/25\n",
      "1384/1384 [==============================] - 17s 12ms/step - loss: 0.0431 - accuracy: 0.9843 - val_loss: 0.3446 - val_accuracy: 0.9371\n",
      "Epoch 11/25\n",
      "1384/1384 [==============================] - 17s 12ms/step - loss: 0.0348 - accuracy: 0.9873 - val_loss: 0.3688 - val_accuracy: 0.9349\n",
      "Epoch 12/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0378 - accuracy: 0.9873 - val_loss: 0.3684 - val_accuracy: 0.9374\n",
      "Epoch 13/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0299 - accuracy: 0.9895 - val_loss: 0.4250 - val_accuracy: 0.9345\n",
      "Epoch 14/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0302 - accuracy: 0.9891 - val_loss: 0.4142 - val_accuracy: 0.9407\n",
      "Epoch 15/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0261 - accuracy: 0.9907 - val_loss: 0.4222 - val_accuracy: 0.9374\n",
      "Epoch 16/25\n",
      "1384/1384 [==============================] - 17s 12ms/step - loss: 0.0263 - accuracy: 0.9907 - val_loss: 0.3960 - val_accuracy: 0.9444\n",
      "Epoch 17/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0232 - accuracy: 0.9918 - val_loss: 0.4391 - val_accuracy: 0.9430\n",
      "Epoch 18/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.4403 - val_accuracy: 0.9411\n",
      "Epoch 19/25\n",
      "1384/1384 [==============================] - 17s 12ms/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 0.4252 - val_accuracy: 0.9438\n",
      "Epoch 20/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0197 - accuracy: 0.9929 - val_loss: 0.4695 - val_accuracy: 0.9443\n",
      "Epoch 21/25\n",
      "1384/1384 [==============================] - 16s 11ms/step - loss: 0.0215 - accuracy: 0.9926 - val_loss: 0.4753 - val_accuracy: 0.9435\n",
      "Epoch 22/25\n",
      "1384/1384 [==============================] - 16s 11ms/step - loss: 0.0141 - accuracy: 0.9950 - val_loss: 0.5600 - val_accuracy: 0.9311\n",
      "Epoch 23/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 0.4686 - val_accuracy: 0.9458\n",
      "Epoch 24/25\n",
      "1384/1384 [==============================] - 16s 12ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.5092 - val_accuracy: 0.9451\n",
      "Epoch 25/25\n",
      "1384/1384 [==============================] - 16s 11ms/step - loss: 0.0194 - accuracy: 0.9930 - val_loss: 0.5110 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27c31da9b80>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(X_train_pad, y_train, batch_size=32, epochs=25, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sarcasm_cnn(s):\n",
    "    x_final = pd.DataFrame({\"headline\":[s]})\n",
    "    test_lines = CleanTokenize(x_final)\n",
    "    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
    "    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "    pred = cnn.predict(test_review_pad)\n",
    "    pred*=100\n",
    "    if pred[0][0]>=50: return \"It's a sarcasm!\" \n",
    "    else: return \"It's not a sarcasm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 93ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm_cnn(\"You just saved my dog's life.Sorry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm_cnn(\"I don't know your name, Andrew?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm_cnn(\"You just broke my car window. Great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sarcasm Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
